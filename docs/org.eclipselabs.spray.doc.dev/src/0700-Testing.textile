h1. Testing

h2. Test Projects

All test projects can be found in the repository in the @tests@ folder.

h2. Test aspects

There are several aspects of Spray that have to be covered by tests:

h3. Testing using the Spray DSL Editor enviroment

h4. Testing the wizard 

* Does the Spray wizard offer the expected configuration options?
* Does the Spray wizard produce a project with the expected configuration?
** plugin.xml
** MANIFEST.MF
** source folders
** created diagram and domain model with the expected file extensions
** does the properties file exist and contain the expected values  

h4. Testing the Spray DSL editor 

* Is the imported domain model resolved?
* Do changes in the DSL editor trigger the code generation?
* Are semantic errors marked and probably even quick fixes offered for them (producing the expected change)?
* Do the proposals work? 
** Are the predefined DSL templates for e.g. containers and connections offered?
** Is the color picker shown?
** Is access provided to JVM types in the class path?
* Do cross references inside the DSL work (e.g. referencing an already defined behavior group)?
* Does the formatter produce the expected output?
* After save, is the DSL serialized correctly and can it be parsed afterwards?

h4. Testing the generated code 

* Are all expected artifact generated for a given DSL?
** correct package?
** correct name?
** base class as well as gap class generated?
** expected file content?
* Does the generated code compile?
* Is a warning dialog shown, when it is tried to edit a generated file?
* Is there a context menu entry for moving a gap class from src-gen to src and does it work?
* Is a gap class not regenerated when it has been moved to src?

h3. Testing the generated Graphiti editor

* Can the Graphiti editor runtime be started out of the generated code?
* Can a new diagram with the expected diagram be created?
* Does the generated Graphiti editor provide its expected abilities in compliance to what has been defined in the DSL? 
* Can all workflows when using the generated Graphiti editor run through without failure?
* Does the palette contains all expected elements in the expected palette compartments?
* *Creating elements*: Does the domain model file contain a corresponding object with the expected properties after an element has been created in the diagram editor?  
* *Adding elements*: Is the expected graphical representation with the expected properties created in the diagram model after the element from the palette has been selected and droped in the editor?
* *Removing elements*: Is the graphical representation removed without removing the domain element?
* *Updating elements*: Are changes to the domain object done outside the diagram editor recognized in the diagram editor?
* *Deleting elements*: Are the graphical representation as well as the domain element are deleted from their models?
* *Resizing and layouting*: Does the diagram elements have the expected coordinates after resizing (e.g. when a rectangle is resized, the contained text is aligned, too)?
* *Constraints*: Are the constraints applied when creating, adding, moving and resizing diagram elements (e.g. no connection allowed between certain objects)?
* *Context buttons and menus*: Are the expected context buttons and context menu entries available at the diagram elements?
* *Properties View*: Is the properties view shown when clicking on diagram elements and does it show the expected fields containing the expected values?
* Is the editor shown as dirty when there are changes in the diagram?
* When saved, are all changes persisted in the diagram file as well as in the domain model file and then the editor is not shown as dirty? 


h2. Test approaches

h3. JUnit tests

* should be used when a functionality can be tested without the need to start an Eclipse instance (as they do not need a certain state only reachable by a running Eclipse)
* this of kind test should be preferred 
* we use JUnit4
* common test logic should be factored out, so that the actual test method states only the test parameters and passes them to a common test executer method, this makes the tests more readable
* following the suggestions of the XUnit Test Pattern book, the name of a test method should contain
** test as prefix
** the name of the method to test, e.g. testGetSomething
** optional the condition under that the test is executed, e.g. testGetSomething_WhenNullIsPassed
** optional the expected result, e.g. testGetSomething_WhenNullIsPassed__NullExpected
* the instance of the class to to test should be declared as global variable and assigned in a with @Before annotated setup method   
* in a Maven pom these test should be execute either with the Maven-Surefire plugin or as alternative with the Tycho-Surefire plug-in, but then without the use of the UI test harness

h3. Mocking

* to improve the isolation of tests but also as a way to avoid the need of a running Eclipse in some cases it is sensible to use a mocking framework to stub the interactions of the class to test
* we propose the use of "Mockito":http://code.google.com/p/mockito/ in conjunction with "Powermock":http://code.google.com/p/powermock/ as mocking framework, these libraries are exported by the plug-in _org.eclipselabs.spray.testhelper_ and should be added to the MANIFEST.MF dependencies of the test plug-in if it want to use these mocking frameworks
* with Mockito you can stub interfaces as well as classes and put them in place of the classes the class under test would usually interact with, you can specify which values should be returned when a method of the stubbed class is called with certain parameters
* with Powermock the abilities of Mockito are extended to support mocking e.g. static method calls as well as constructor calls
* as most classes in Spray uses Google Guice injection, it is easy to inject the stubbed classes in place of the real classes, in all other cases sub classing the class to test may be an option   

h3. Xtext tests

* tests that exercises Xtext based classes should define the Xtext runner at the class header: @RunWith(XtextRunner.class)
* to replace the default injection configuration define a custom injection provider and also declare it in the header of the test class: @InjectWith(SprayTestsInjectorProvider.class)
* after that you are able inject the classes inside the test class itself as well as use them in the classes to test
* to ease the writting of tests for Xtext based code, the xtext-utils are used
* beside checking the parsing and serializing of DSL models you can also test things like scoping and validation
* for further insight in how to test Xtext DSLs also checkout Moritz Eysholdt's "presentation":http://www.slideshare.net/meysholdt/testdriven-development-of-xtext-dsls covering that topic
* Xtext tests can be executed headless, i.e. as JUnit tests

h3. JUnit plug-in tests

* in some cases it inevitable to start up an Eclipse instance to be able to execute tests successfully, as these tests depend on state only available in a running Eclipse (e.g. state is set by the Activator of the plug-in or the test queries the existing perspectives)
* some error states are not reproducable as JUnit test but only in a JUnit plug-in test (e.g. being able to import the domain model)
* some other error states only occur in a Eclipse RCP product having the Spray feature installed (e.g. build.properties does not include all required files) 
* in a Maven pom these test should be execute with the Tycho-Surefire plug-in with setting the use of the UI test harness to true
* note that those non headless tests will not work on the Jenkins build server hosted by CloudBees, as this host on Linux machine, so their execution is disable there

h3. UI tests

* as extension of JUnit plug-in tests UI tests executes operations a user would do usually manually, e.g. open menus, clicking on buttons, but also dragging elements in the Graphiti editor, do resize and so on
* we currently use SWT Bot, to define those tests, in contrast to a tool like WindowTester it is not able to record interactions with the UI, so you have to program those interactions by your own
* UI tests tend to be brittle and are hard to read and maintain, so they should be use when the functionality to test cannot be coverage by other test means
** depend on the performance of the Computer where they are executed (reactions too slow or too fast, have to work with sleeps and cope with threading issues)
** behave sometimes different on different platforms (UI elements cannot be found that have been found in a test run under a different operating system)

h3. Measuring test coverage

* there has been added support of measuring code coverage in the Eclipse development environment as well as in the Maven build with using EMMA
* there are class loading issues when the code to instrument by EMMA uses Google Guice injection, so test coverage is currently mostly measure only for the test code that is excecuted but not for the code that is exercised by the test code (that would be actually the more interesting code coverage to measure)

h3. Test data organization

* example domain models are provided in the model folder of the test plug-in
* the test models are named, numbered and organized in different packages to better associate them with the tests that uses them
* for smaller tests it may sufficient to build up the test models with using the generated EMF factory class instead of providing extra test models
* the test models can be used to compare expected and actual output, e.g. when testing the Spray DSL formatter
* beside the test DSLs there can be also expected generator artifacts provided to be used in output comparison assertions   

h3. Generator template tests

* beside testing the generated code by executing it and check its behavior, it is sometimes more sensible to additional provide tests for generator templates and helper methods, to be able to spot more easily where are failures in the generator logic, as some of those failures only occur under certain circumstances
* so instead of executing the hole generator with a bunch of different models, single templates and helper methods can be execute with several variations of model parts only applicable to the template or helper method to test and verify their outputs
* you can check fault-tolerant does the templates and extensions act to invalid or incomplete model data  

h2. Spray Language Tests

The Spray Language is tested in project @org.eclipselabs.spray.xtext.tests@. This project makes use of the "xtext-utils unittesting":http://code.google.com/a/eclipselabs.org/p/xtext-utils/wiki/Unit_Testing framework. These tests use example models that are read and processed and cover the following topics:
* Parsing of the models
* Resolving cross references / scoping
* Validation
* Formatter
* Serializing

Test models are kept below the @/model@ folder. The examples should be as minimal as possible to demonstrate and test a language feature. This is especially important for debugging, since effort for debugging grows with the size of models.

The main test class is @org.eclipselabs.spray.xtext.tests.ModelTests@. The initialization part 

bc.. 
@RunWith(XtextRunner2.class)
@InjectWith(SprayTestsInjectorProvider.class)
public class ModelTests extends XtextTest {
    @Before
    public void before() {
        super.before();
        suppressSerialization();
        EPackage.Registry.INSTANCE.put(GenModelPackage.eNS_URI, GenModelPackage.eINSTANCE);
        EcorePlugin.getEPackageNsURIToGenModelLocationMap().put(BusinessDomainDslPackage.eNS_URI, URI.createURI("classpath:/mod4j/BusinessDomainDsl.genmodel"));
    }


    ...
}
p. 

A test method is simply invoking the @testFile()@ method and passes the file under test. If additional resources are required to resolve cross references (typically at least the Ecore file used as metamodel) these are passed as additional arguments.

bc.. 
    @Test
    public void test_20_color() {
        testFile("testcases/20-color.spray", "mod4j/BusinessDomainDsl.ecore");
    }
p. 

See also the slides "Test-Driven Development of Xtext DSLs":http://www.slideshare.net/meysholdt/testdriven-development-of-xtext-dsls.


h2. Runtime Tests

The Spray generator components tests can be found in project @org.eclipselabs.spray.generator.graphiti.tests@. 



